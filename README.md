# Korean Title Generator

Final Project in 2nd BoostCamp AI Tech 2ê¸° by **ë©”íƒ€ëª½íŒ€ (2ì¡°)**

## Demo
![Demo](https://github.com/boostcampaitech2/final-project-level3-nlp-02/blob/dev/assets/%EC%8B%9C%EC%97%B0%EC%98%81%EC%83%81.gif?raw=true)


## Content
- [Project Abstract](#project-abstract)
- [How to use (ìµœì¢… ëª¨ë¸ checkpointë¡œ ìˆ˜ì •í•´ì•¼í•¨)](#how-to-use--------checkpoint--------)
- [Result (ê²°ê³¼ ë½‘ê³  ìˆ˜ì • í•„ìš”)](#result--------------)
- [Hardware](#hardware)
- [Operating System](#operating-system)
- [Archive Contents](#archive-contents)
- [Getting Started](#getting-started)
  * [Dependencies](#dependencies)
  * [Install Requirements](#install-requirements)
- [Arguments](#arguments)
  * [Model Arguments](#model-arguments)
  * [DataTrainingArguments](#datatrainingarguments)
  * [LoggingArguments](#loggingarguments)
  * [GenerationArguments](#generationarguments)
  * [Seq2SeqTrainingArguments](#seq2seqtrainingarguments)
- [Running Command](#running-command)
  * [Train](#train)
  * [Predict](#predict)
- [Reference](#reference)


## Project Abstract

ðŸ”¥ ìƒì„± ìš”ì•½ì„ í†µí•œ í•œêµ­ì–´ ë¬¸ì„œ ì œëª© ìƒì„±ê¸° ðŸ”¥
- ë°ì´í„°ì…‹ :
  - ì¢…ë¥˜ : [ë…¼ë¬¸ ë°ì´í„°](https://aihub.or.kr/aidata/30712) 162,341ê°œ, [ë¬¸ì„œ ë°ì´í„°](https://aihub.or.kr/aidata/8054) 371,290ê°œ
  - train_data : 275,219ê°œ (Text, Title, Document Type)
  - validation_data : 91,741ê°œ (Text, Title, Document Type)
  - test_data : 81,739ê°œ (Text, Title, Document Type)

## How to use (ìµœì¢… ëª¨ë¸ checkpointë¡œ ìˆ˜ì •í•´ì•¼í•¨)

``` python
import torch
from transformers import AutoConfig
from transformers import AutoTokenizer
from models.modeling_kobigbird_bart import EncoderDecoderModel

config = AutoConfig.from_pretrained('metamong1/bigbird-tapt-ep3')
tokenizer = AutoTokenizer.from_pretrained('metamong1/bigbird-tapt-ep3')
model = EncoderDecoderModel.from_pretrained('metamong1/bigbird-tapt-ep3', config=config)

text = "ë³¸ ë…¼ë¬¸ì˜ ëª©ì ì€ ìˆ˜ë„ê¶Œ ì§€ì—­ì˜ ìˆ˜ì¶œìž… ì»¨í…Œì´ë„ˆ í™”ë¬¼ì— ëŒ€í•œ ìµœì  ë³µí•©ìš´ì†¡ ë„¤íŠ¸ì›Œí¬ë¥¼ ì°¾ëŠ” ë° ìžˆë‹¤. ë”°ë¼ì„œ ì´ ì§€ì—­ì˜ ì»¨í…Œì´ë„ˆ í™”ë¬¼ì˜ ë¬¼ë™ëŸ‰ íë¦„ì„ ìš°ì„ ì ìœ¼ë¡œ ë¶„ì„í•˜ì˜€ê³ , ì´ ìˆ˜ì†¡ë¹„ìš©ê³¼ ì´ ìˆ˜ì†¡ ì‹œê°„ì„ ê³ ë ¤í•œ ìµœì  ê²½ë¡œë¥¼ êµ¬í•˜ë ¤ ì‹œë„í•˜ì˜€ë‹¤. ì´ë¥¼ ìœ„í•´ ëª¨í˜• ì„¤ì •ì€ 0-1 ì´ì§„ë³€ìˆ˜ë¥¼ ì´ìš©í•œ ëª©ì ê³„íšë²•ì„ ì‚¬ìš©í•˜ì˜€ê³ , ìœ ì „ì•Œê³ ë¦¬ì¦˜ ê¸°ë²•ì„ í†µí•´ í•´ë¥¼ ë„ì¶œí•˜ì˜€ë‹¤. ê·¸ ê²°ê³¼, ìˆ˜ë„ê¶Œ ì§€ì—­ì˜ 33ê°œ ê° ì‹œ(êµ°)ì— ëŒ€í•œ ë‚´ë¥™ ìˆ˜ì†¡ë¹„ìš©ê³¼ ìˆ˜ì†¡ ì‹œê°„ì„ ìµœì†Œí™”í•˜ëŠ” ìˆ˜ì†¡ê±°ì  ë° ìš´ì†¡ ìˆ˜ë‹¨ì„ ë„ì¶œí•¨ìœ¼ë¡œì¨ ì´ ì§€ì—­ì˜ ìˆ˜ì¶œìž… ì»¨í…Œì´ë„ˆ í™”ë¬¼ì— ëŒ€í•œ ìµœì  ë³µí•©ìš´ì†¡ ë„¤íŠ¸ì›Œí¬ë¥¼ ë°œê²¬í•  ìˆ˜ ìžˆì—ˆë‹¤. ë˜í•œ ì‹œë‚˜ë¦¬ì˜¤ë³„ ìˆ˜ì†¡ë¹„ìš© ë° ìˆ˜ì†¡ ì‹œê°„ì˜ ì ˆê° íš¨ê³¼ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì œì‹œí•œë‹¤."

raw_input_ids = tokenizer.encode(text)
input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]

summary_ids = model.generate(torch.tensor([input_ids]))
tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)
```

ì •ë‹µ ì œëª©
> ìœ ì „ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•œ ë³µí•©ìš´ì†¡ìµœì í™”ëª¨í˜•ì—ê´€í•œ ì—°êµ¬

ìƒì„± ì œëª©
> ìœ ì „ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•œ ìˆ˜ë„ê¶Œì˜ ìˆ˜ì¶œìž… ì»¨í…Œì´ë„ˆ í™”ë¬¼ì— ëŒ€í•œ ìµœì  ë³µí•©ìš´ì†¡



## Result (ê²°ê³¼ ë½‘ê³  ìˆ˜ì • í•„ìš”)

|            | RougeL |
|:----------:|:------:|
| Evaluation | ------ |
|    Test    | ------ |


## Hardware

- Intel(R) Xeon(R) Gold 5120 CPU @ 2.20GHz
- NVIDIA Tesla V100-SXM2-32GB

## Operating System

- Ubuntu 18.04.5 LTS

## Archive Contents

- final-project-level3-nlp-02 : êµ¬í˜„ ì½”ë“œì™€ ëª¨ë¸ checkpoint ë° ëª¨ë¸ ê²°ê³¼ë¥¼ í¬í•¨í•˜ëŠ” ë””ë ‰í† ë¦¬

```
final-project-level3-nlp-02/
â”œâ”€â”€ model
â”‚   â”œâ”€â”€ args  
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚Â Â  â”œâ”€â”€ DataTrainingArguments.py
â”‚   â”‚ Â  â”œâ”€â”€ GenerationArguments.py
â”‚   â”‚Â Â  â”œâ”€â”€ LoggingArguments.py
â”‚   â”‚Â Â  â”œâ”€â”€ ModelArguments.py
â”‚   â”‚Â Â  â””â”€â”€ Seq2SeqTrainingArguments.py
â”‚   â”œâ”€â”€ models  
â”‚   â”‚   â”œâ”€â”€ modeling_distilbert.py
â”‚   â”‚Â Â  â”œâ”€â”€ modeling_kobigbird_bart.py
â”‚   â”‚Â Â  â””â”€â”€ modeling_longformer_bart.py
â”‚   â”œâ”€â”€ utils   
â”‚   â”‚   â”œâ”€â”€ data_collator.py
â”‚   â”‚Â Â  â”œâ”€â”€ data_loader.py
â”‚   â”‚ Â  â”œâ”€â”€ data_preprocessor.py
â”‚   â”‚Â Â  â”œâ”€â”€ rouge.py
â”‚   â”‚Â Â  â””â”€â”€ trainer.py
â”‚   â”œâ”€â”€ optimization   
â”‚   â”‚   â”œâ”€â”€ knowledge_distillation.py
â”‚   â”‚Â Â  â”œâ”€â”€ performance_test.py
â”‚   â”‚ Â  â”œâ”€â”€ performnaceBenchmark.py
â”‚   â”‚Â Â  â””â”€â”€ quantization.py
â”‚   â”œâ”€â”€ predict.py
â”‚   â”œâ”€â”€ pretrain.py
â”‚   â”œâ”€â”€ REDAME.md
â”‚   â”œâ”€â”€ running.sh
â”‚   â””â”€â”€ train.py
â”œâ”€â”€ serving
â”‚Â Â  â”œâ”€â”€ app.py
â”‚Â Â  â”œâ”€â”€ GenerationArguments.py
â”‚Â Â  â”œâ”€â”€ postprocessing.py
â”‚Â Â  â”œâ”€â”€ predict.py
â”‚Â Â  â”œâ”€â”€ utils.py
â”‚Â Â  â””â”€â”€ viz.py
â”œâ”€â”€.gitignore
â””â”€â”€requirements.sh

```

- `utils`
    - `utils/data_collator.py` : ëª¨ë¸ì— ìž…ë ¤ë˜ëŠ” Batchë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œ
    - `utils/data_preprocessor.py` : Textë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” ì½”ë“œ
    - `utils/processor.py` : Textë¥¼ Tokenizerë¥¼ ì´ìš©í•´ì„œ ì •ìˆ˜ ì¸ì½”ë”©ì„ í•˜ëŠ” ì½”ë“œ
    - `utils/rouge.py` : ëª¨ë¸ì˜ í‰ê°€ì§€í‘œì™€ ê´€ë ¨ë˜ëŠ” ì½”ë“œ
    - `utils/trainer.py` : ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ”ë° í™œìš©í•˜ëŠ” trainer ì½”ë“œ
- `models`
    - `modeling_kobigbird_bart.py` : bigbird bart ëª¨ë¸ ì½”ë“œ
    - `modeling_longformerbart.py` : longformer bart ëª¨ë¸ ì½”ë“œ
- `optimization`
    - `knowledge_distillation.py` :
    - `performanceBenchmark.py` : 
    - `performance_test.py` : 
    - `quantization.py` : 
- `predict.py` : ëª¨ë¸ì„ ì´ìš©í•´ì„œ ìž…ë ¥ëœ ë¬¸ì„œì˜ ì œëª©ì„ ìƒì„±í•˜ëŠ” ì½”ë“œ
- `pretrain.py` : summarization modelì„ fintuneì„ ìœ„í•œ ì½”ë“œ
- `train.py` : summarization modelì„ pretrainì„ ìœ„í•œ ì½”ë“œ

## Getting Started

### Dependencies

- python=3.8.5
- transformers==4.11.0
- datasets==1.15.1
- torch==1.10.0
- streamlit==1.1.0
- elasticsearch==7.16.1
- pyvis==0.1.9
- plotly==5.4.0


### Install Requirements

```
sh requirements.sh
```

## Arguments
    
### Model Arguments

|   argument | description    | default         |
| :--------: | :----------------------- | :------------- |
|   model_name_or_path   | ì‚¬ìš©í•  ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ì„ íƒ   | gogamza/kobart-base-v1   |
| use_model | ëª¨ë¸ íƒ€ìž… ì„ íƒ [auto, bigbart, longbart] | auto |
|   config_name    | Pretrainedëœ model config ê²½ë¡œ  | None |
|     tokenizer_name     | customized tokenizer ê²½ë¡œ ì„ íƒ   | None    |
| use_fast_tokenizer | fast tokenizer ì‚¬ìš© ì—¬ë¶€ | True |
| hidden_size | embedding hidden dimension í¬ê¸° | 128 |

ðŸ‘‡ longBART specific
|      argument       | description   | default      |
| :-----------------: | :------------ | :----------- |
| attention_window_size | attention window í¬ê¸° | 256 |
| attention_head_size | attention head ê°œìˆ˜ | 4 |
| encoder_layer_size | encoder layer ìˆ˜ | 3 |
| decoder_layer_size | decoder layer ìˆ˜ | 3 |



### DataTrainingArguments

|    argument  | description | default    |
| :-------: | :------------ | :------- |
| text_column | datasetì—ì„œ ë³¸ë¬¸ column ì´ë¦„ | text |
| title_column | datasetì—ì„œ ì œëª© column ì´ë¦„ | title |
| overwrite_cache |  ìºì‹œëœ trainingê³¼ evaluation setì„ overwriteí•˜ê¸° | False |
| preprocessing_num_workers | ì „ì²˜ë¦¬ë™ì•ˆ ì‚¬ìš©í•  prcoess ìˆ˜ ì§€ì • | 1 |
| max_source_length | Text Sequence ê¸¸ì´ ì§€ì • | 1024 |
| max_target_length | Title Sequence ê¸¸ì´ ì§€ì • | 128 |
| pad_to_max_length | ìµœëŒ€ ê¸¸ì´ë¡œ Paddingì„ ì§„í–‰ | False |
| num_beams | Evaluation í•  ë•Œì˜ beam searchì—ì„œ beamì˜ í¬ê¸° | None |
| use_auth_token_path | Huggingfaceì— Datasetì„ í˜¹ì€ Modelì„ ë¶ˆëŸ¬ì˜¬ ë•Œ private key ì£¼ì†Œ | ./use_auth_token.env |
| num_samples | train_datasetì—ì„œ sample ì¶”ì¶œ ê°¯ìˆ˜(Noneì¼ ë•ŒëŠ” ì „ì²´ ë°ì´í„° ìˆ˜ ì‚¬ìš©) | None |
| relative_eval_steps| Evaluation íšŸìˆ˜ | 10 |
| is_pretrain| Pretraining ì—¬ë¶€ | False |
| is_part | ì „ì²´ ë°ì´í„° ìˆ˜ì˜ 50% ì •ë„ ì‚¬ìš© | False |
| use_preprocessing | ì „ì²˜ë¦¬ ì—¬ë¶€ | False |
| use_doc_type_ids | doc_type_embedding ì‚¬ìš© ì—¬ë¶€ | False |


### LoggingArguments

|     argument          | description         | default        |
| :------:     | :-------------- | :------------- |
| wandb_unique_tag | wandbì— ê¸°ë¡ë  ëª¨ë¸ì˜ ì´ë¦„ | None |
| dotenv_path | wandb keyê°’ì„ ë“±ë¡í•˜ëŠ” íŒŒì¼ì˜ ê²½ë¡œ  | ./wandb.env |
| project_name | wandbì— ê¸°ë¡ë  project name | Kobart |

### GenerationArguments

| argument | description  | default |
| :----: | :--------- | :------------- |
|    max_length        | ìƒì„±ë  ë¬¸ìž¥ì˜ ìµœëŒ€ ê¸¸ì´    | None  |
| min_length    | ìƒì„±ë  ë¬¸ìž¥ì˜ ìµœì†Œ ê¸¸ì´  | 1 |
| length_penalty | ë¬¸ìž¥ì˜ ê¸¸ì´ì— ë”°ë¼ ì£¼ëŠ” penaltyì˜ ì •ë„ | 1.0 |
|    early_stopping        | Beamì˜ ê°¯ìˆ˜ ë§Œí¼ ë¬¸ìž¥ì˜ ìƒì„±ì´ ì™„ë£Œ ë˜ì—ˆì„ ë•Œ ìƒì„±ì„ ì¢…ë£Œ ì—¬ë¶€   | True  |
|    output_scores        | prediction score ì¶œë ¥ ì—¬ë¶€    | False  |
|    no_repeat_ngram_size        | ë°˜ë³µ ìƒì„±ë˜ì§€ ì•Šì„ ngramì˜ ìµœì†Œ í¬ê¸° | 3  |
| num_return_sequences | ìƒì„± ë¬¸ìž¥ ê°¯ìˆ˜ | 1 |
| top_k    | Top-K í•„í„°ë§ì—ì„œì˜ K ê°’  | 50 |
| top_p | ìƒì„± ê³¼ì •ì—ì„œ ì´ì–´ì§€ëŠ” í† í°ì„ ì„ íƒí•  ë•Œì˜ ìµœì†Œ í™•ë¥  ê°’ | 0.95 |

### Seq2SeqTrainingArguments

|     argument  | description           | default        |
| :-----:  | :---------- | :------------- |
| metric_for_best_model  | train í›„ ì €ìž¥ë  ëª¨ë¸ ì„ ì • ê¸°ì¤€ | rougeL  |
| es_patience  | early stoppingì´ ë˜ëŠ” patience ê°’ | 3  |
| is_noam  | noam scheduler ì‚¬ìš© ì—¬ë¶€  | False |
| use_rdrop | R-drop ì‚¬ìš© ì—¬ë¶€ | False |
| reg_alpha | R-drop ì‚¬ìš© ì‹œ ì ìš©ë  KL loss ë¹„ìœ¨ | 0.7 |
| alpha | knowledge distillation ì‹œ CE loss ì ìš© ë¹„ìœ¨ | 0.5 |
| temperature | distillationì„ í•  ë•Œì˜ temperature ê°’ | 1.0 |
| use_original | tiny distillationì„ í•  ë•Œ prediction loss ì‚¬ìš© ì—¬ë¶€ | False |
| teacher_check_point | teacher modelì˜ checkpoint | None |
| use_teacher_forcing | teacher forcing ì ìš© ì—¬ë¶€ | False |


## Running Command
### Train
```
$ python train.py \
--model_name_or_path metamong1/bigbird-tapt-ep3 \
--use_model bigbart \
--do_train \
--output_dir checkpoint/kobigbirdbart_full_tapt_ep3_bs16_pre_noam \
--overwrite_output_dir \
--num_train_epochs 3 \
--use_doc_type_ids \
--max_source_length 2048 \
--max_target_length 128 \
--metric_for_best_model rougeLsum \
--es_patience 3 \
--load_best_model_at_end \
--project_name kobigbirdbart \
--wandb_unique_tag kobigbirdbart_full_tapt_ep5_bs16_pre_noam \
--per_device_train_batch_size 2 \
--per_device_eval_batch_size 16 \
--gradient_accumulation_steps 8 \
--use_preprocessing \
--warmup_steps 1000 \
--evaluation_strategy epoch \
--is_noam \
--learning_rate 0.08767941605644963 \
--save_strategy epoch
```

### Predict

```
$ python predict.py \
--model_name_or_path model/baseV1.0_Kobart_ep2_1210 \
--num_beams 3
```


## Reference
1. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
    > https://arxiv.org/pdf/1910.13461.pdf
2. Longformer: The Long-Document Transformer 
    > https://arxiv.org/pdf/2004.05150.pdf
3. Big Bird: Transformers for Longer Sequences
    > https://arxiv.org/pdf/2007.14062.pdf
4. Scheduled Sampling for Transformers 
    > https://arxiv.org/pdf/1906.07651.pdf
5. On the Effect of Dropping Layers of Pre-trained Transformer Models
    > https://arxiv.org/pdf/2004.03844.pdf
6. R-Drop: Regularized Dropout for Neural Networks
    > https://arxiv.org/pdf/2106.14448v2.pdf
